<!DOCTYPE html>
<html lang="en"><head>
  <meta name="norton-safeweb-site-verification" content="90pl-9ylotk4t6ak5p4l0ybq5hiiie5na-z3q-nyea-jloyjmbxk3l3toywo-xxqy0y6-9b-oyc01k1w0-tfy8pu512c2t-weaab3exdbotwnp67mx01pmffe7ehe3-j" />
  
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
  <link rel="manifest" href="/manifest.webmanifest"/>
  <meta name="theme-color" content="#333"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <link rel="canonical" href="https://nickmaltbie.com"/>

  <!-- icon in the highest resolution we need it for -->
  <link rel="icon" sizes="192x192" href="logo_192.png"/>
  <!-- reuse same icon for Safari -->
  <link rel="apple-touch-icon" href="logo_512.jpg"/>
  <!-- multiple icons for IE -->
  <meta name="msapplication-square310x310logo" content="logo_310.jpg"/>

  <script async src="/assets/scripts/showMores.js"></script>
  
  <script>
  // Check that service workers are registered
  if ('serviceWorker' in navigator) {
    // Use the window load event to keep the page load performant
    window.addEventListener('load', () => {
      navigator.serviceWorker.register('/service-worker.js');
    });
  }
  </script><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Research Assistant | Nick Maltbie</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Research Assistant" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites. Image of polyview identification of protein protein interaction sites (pink) for test sequence. This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer. Experimenting With Tensor Flow to Predict BSI Results In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation. After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve. In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement. Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard. Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation). Strucutre of DNN Strucutre of RNN" />
<meta property="og:description" content="I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites. Image of polyview identification of protein protein interaction sites (pink) for test sequence. This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer. Experimenting With Tensor Flow to Predict BSI Results In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation. After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve. In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement. Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard. Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation). Strucutre of DNN Strucutre of RNN" />
<link rel="canonical" href="https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html" />
<meta property="og:url" content="https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html" />
<meta property="og:site_name" content="Nick Maltbie" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-08-01T00:00:00+00:00" />
<script type="application/ld+json">
{"description":"I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites. Image of polyview identification of protein protein interaction sites (pink) for test sequence. This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer. Experimenting With Tensor Flow to Predict BSI Results In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation. After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve. In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement. Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard. Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation). Strucutre of DNN Strucutre of RNN","url":"https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html"},"headline":"Research Assistant","dateModified":"2017-08-01T00:00:00+00:00","datePublished":"2017-08-01T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link async rel="stylesheet" href="/assets/main.css"/><link type="application/atom+xml" rel="alternate" href="https://nickmaltbie.com/feed.xml" title="Nick Maltbie" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Research Assistant | Nick Maltbie</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Research Assistant" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites. Image of polyview identification of protein protein interaction sites (pink) for test sequence. This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer. Experimenting With Tensor Flow to Predict BSI Results In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation. After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve. In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement. Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard. Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation). Strucutre of DNN Strucutre of RNN" />
<meta property="og:description" content="I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites. Image of polyview identification of protein protein interaction sites (pink) for test sequence. This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer. Experimenting With Tensor Flow to Predict BSI Results In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation. After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve. In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement. Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard. Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation). Strucutre of DNN Strucutre of RNN" />
<link rel="canonical" href="https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html" />
<meta property="og:url" content="https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html" />
<meta property="og:site_name" content="Nick Maltbie" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-08-01T00:00:00+00:00" />
<script type="application/ld+json">
{"description":"I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites. Image of polyview identification of protein protein interaction sites (pink) for test sequence. This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer. Experimenting With Tensor Flow to Predict BSI Results In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation. After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve. In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement. Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard. Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation). Strucutre of DNN Strucutre of RNN","url":"https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nickmaltbie.com/professional/2017/08/01/CCHMC-Research.html"},"headline":"Research Assistant","dateModified":"2017-08-01T00:00:00+00:00","datePublished":"2017-08-01T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>
  <script src="/assets/scripts/top_button.js"></script>

  <script src="/assets/lib/jquery.min.js"></script>
  <style>
    #cookie-notice {
        z-index: 10000;
        padding: 7px 15px;
        display: none;
        text-align: center;
        position: fixed;
        bottom: 0;
        background: #222;
        color: rgba(255,255,255,0.8);
    }
    #cookie-notice a {
        cursor: pointer;
        margin-left: 10px;
    }
    @media (max-width: 767px) {
        #cookie-notice span {display: block; padding-top: 3px; margin-bottom: 13px;}
        #cookie-notice a {position: relative; bottom: 4px;}
    }
    @media (min-width: 767px) {
        #cookie-notice {width: 100vw;}
    }
</style>
<div id="cookie-notice"><span>We would like to use third party cookies and scripts to improve the functionality of this website.</span><a id="cookie-notice-accept" class="btn btn-primary btn-sm">Approve</a><a href="/privacy.html" class="btn btn-primary btn-sm">Privacy Policy</a></div>
<script>
    function createCookie(name,value,days) {
        var expires = "";
        if (days) {
            var date = new Date();
            date.setTime(date.getTime() + (days*24*60*60*1000));
            expires = "; expires=" + date.toUTCString();
        }
        document.cookie = name + "=" + value + expires + "; path=/";
    }
    function readCookie(name) {
        var nameEQ = name + "=";
        var ca = document.cookie.split(';');
        for(var i=0;i < ca.length;i++) {
            var c = ca[i];
            while (c.charAt(0)==' ') c = c.substring(1,c.length);
            if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length,c.length);
        }
        return null;
    }
    function eraseCookie(name) {
        createCookie(name,"",-1);
    }

    if(readCookie('cookie-notice-dismissed')=='true') {

    } else {
        $('#cookie-notice').show();
    }
    $('#cookie-notice-accept').click(function() {
        createCookie('cookie-notice-dismissed','true',31);
        $('#cookie-notice').hide();
        location.reload();
    });

</script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-134298967-1"></script>
<script>
	if(readCookie('cookie-notice-dismissed')=='true') {
	  	window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-134298967-1');
	}
</script>


</head>
<body><header class="site-header" role="banner">

  <link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet"/>

  <div class="header-wrapper"><a class="site-title" rel="author" href="/">Nick Maltbie</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          <a class="page-link" href="/">Home</a>
          <a class="page-link" href="/about.html">About Me</a>
          <a class="page-link" href="/blog.html">Blog</a>
          <a class="page-link" href="/professional.html">Professional</a>
          <a class="page-link" href="/honors.html">Honors</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
  <div class="wrapper">
    <header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">Research Assistant</h1>
  <p class="post-meta">
    <time class="dt-published" datetime="2017-08-01T00:00:00+00:00" itemprop="datePublished">Aug 1, 2017
    </time></p>
</header>

<div class="post-content e-content" itemprop="articleBody">
  <div class="post-block"><div class="post-element">
  <h3>Skills Used</h3>
  <ul class="skills-container">
  <li class="skill">
      Bash
    </li>
  <li class="skill">
      C++
    </li>
  <li class="skill">
      Linux
    </li>
  <li class="skill">
      Machine Learning
    </li>
  <li class="skill">
      Python
    </li>
  <li class="skill">
      R
    </li>
  
  </ul>
</div>
<div class="post-element">May 2017 to Aug 2017
at Cincinnati Childrens Hospital Medical Center
</div></div>
<p>I created an automated pipeline to generate covariance data about protein sequences (CoeViz: a web-based tool for coevolution analysis of protein residues. Frazier Baker and Aleksey Porollo). Covariance data describes the evolutionary profile of a protein, this is useful for determining interaction sites on proteins and relates to protein folding and function. The data processed was open source data: the Baker’s Yeast protein set from National Center for Biotechnology Information (NCBI) database and the BioLiP data is from Jianyi Yang, Ambrish Roy and Yang Zhang at the Department of Computational Medicine and Bioinformatics, University of Michigan. The Baker’s Yeast dataset is simply a collection of protein sequences and the BioLiP dataset is a set of labeled sequences with information about structure and interaction sites.</p>

<p><img src="/assets/projects/cchmc-2017/cchmc-1.png" alt="Polyview Render" /></p>

<p>Image of polyview identification of protein protein interaction sites (pink) for test sequence.</p>

<p>This pipeline is composed of bash macros that call python, R, and perl scripts that I adapted or wrote and some more advanced programs that Dr. Porollo has created in his research. This data can potentially be used by machine learning to predict interaction sites on proteins in the future. As part of the pipeline, I first used Basic Local Alignment Search Tool (BLAST) to reduce redundancy in the dataset. These resulting sequences were then run through comparison analysis against a larger database and the results were aligned and interpreted to by other programs. This whole process has taken about two months and may not finish before I leave this summer.</p>

<h1 id="experimenting-with-tensor-flow-to-predict-bsi-results">Experimenting With Tensor Flow to Predict BSI Results</h1>

<p>In addition to generating covariance metrics, I also worked with using Tensor Flow to predict patient results based on samples. I was given samples about patients over time and labels for which patients became sick over time. There are only about 100 patients in the dataset so that did not leave much room for error or noise (Majority of the normal are designed to work with thousands or millions of data points). To resolve the limited data, I normalized and filtered for redundant data points. I broke the data up by patient and attempted to group the input vectors. The raw data has over 2,000 values per sample. I normalized all the values between 0 and 1 using various methods and grouped the data based on different conditions. I wrote a few python scripts to manage data generation and a few bash scripts to partially automate data generation.</p>

<p>After the data was normalized, I used various methods of machine learning to predict results based on the data. These implementation were developed using Jupyter Notebook in an Anaconda environment with python. Some methods I used consisted of Kernel Ridge, Linear Support Vector Machine, Linear Ridge, and Stochastic Gradient Descent; all methods found in SciKit learn, a python library. Additionally, I used a simple Dense Neural Network (DNN) with Tensor Flow. These methods all took in a patient’s sample and attempted to determine if the patient is sick. The results were about 60 to 70% accuracy with a high false negative rate. This was alright but without more data points, would be difficult to improve.</p>

<p>In order to make use of the redundant data, I decided to use a Recurrent Neural Network (RNN) and used a Long Short Term Memory (LSTM) cell in Tensor Flow’s library to look at a patient’s records over time. This proved to have a much better true positive rate and an accuracy of around 75%. I am spending time tuning the values to improve the accuracy and quality of the neural network. I am using Jupyter notebook because it makes the adapting the network and adding documentation easy. As of now, there is not enough data to conclude that these methods work for classification but the methods do show promise and possible future improvement.</p>

<p><img src="/assets/projects/cchmc-2017/cchmc-2.png" alt="DNN Learning Graphs" /></p>

<p>Cost (distance from perfect prediction) of LSTM (left) and cost of DNN (right) as they train. Blue line is validation set, orange line is training set. LSTM avoids overfitting better than the DNN. Images generated using Tensorboard.</p>

<p><img src="/assets/projects/cchmc-2017/cchmc-3.png" alt="RNN Learning Graphs" /></p>

<p>Statistics of finalized RNN Network as it trained. Orange line is training set and Blue line is validation set. Lines are smoothed so dim/background colors represent actual points (smoothing reduces the graph’s variation).</p>

<p><img src="/assets/projects/cchmc-2017/cchmc-4.png" alt="DNN Layout" /></p>

<p>Strucutre of DNN</p>

<p><img src="/assets/projects/cchmc-2017/cchmc-5.png" alt="RNN Layout" /></p>

<p>Strucutre of RNN</p>


</div><a class="u-url" href="/professional/2017/08/01/CCHMC-Research.html" hidden></a>

  </div>
</main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">Nick Maltbie</li>
          <li>he/him/his</li><li><a class="u-email" href="mailto:nick.dmalt@gmail.com">nick.dmalt@gmail.com</a>.</li><li>Want to see the <a href="https://github.com/nicholas-maltbie/nick-jekyll">source code</a> of this website.</li>
          <li>View my site's <a class="decorated_link" href="/privacy.html">privacy policy.</a></li>
          <li>See the <a class="decorated_link" href="/references.html">references</a> page for referenced assets.</li>
        </ul>
      </div>

      <div class="footer-col"><ul class="social-media-list"><li>
    <a href="https://github.com/nicholas-maltbie" title="nicholas-maltbie">
      <img width=32px height=32px class="svg-icon" src="/assets/icons/github.svg" alt="github social link"></img>
    </a>
  </li><li>
    <a href="https://www.linkedin.com/in/nicholas-maltbie-b543a8137" title="nicholas-maltbie-b543a8137">
      <img width=32px height=32px class="svg-icon" src="/assets/icons/linkedin.svg" alt="linkedin social link"></img>
    </a>
  </li><li>
    <a href="https://www.twitter.com/NicholasMaltbie" title="NicholasMaltbie">
      <img width=32px height=32px class="svg-icon" src="/assets/icons/twitter.svg" alt="twitter social link"></img>
    </a>
  </li><li>
    <a href="https://www.reddit.com/u/flyQuixote" title="flyQuixote">
      <img width=32px height=32px class="svg-icon" src="/assets/icons/reddit.svg" alt="reddit social link"></img>
    </a>
  </li><li>
    <a href="https://www.youtube.com/channel/UCoGYwhYV-ar1d7b3niqqDQw" title="UCoGYwhYV-ar1d7b3niqqDQw">
      <img width=32px height=32px class="svg-icon" src="/assets/icons/youtube.svg" alt="youtube social link"></img>
    </a>
  </li></ul>
</div>"
    </div>

  </div>

</footer>


              <script>
                window.onload = function () {
                    var script = document.createElement('script');
                    var firstScript = document.getElementsByTagName('script')[0];
                    script.type = 'text/javascript';
                    script.async = true;
                    script.src = '/sw-register.js?v=' + Date.now();
                    firstScript.parentNode.insertBefore(script, firstScript);
                };
            </script>
            </body>


</html>
